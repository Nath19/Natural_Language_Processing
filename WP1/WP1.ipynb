{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Hello everyone.',\n",
       " 'Hope all are fine and doing well.',\n",
       " 'Hope you find the book interesting']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = \" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization of text in other languages :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performing tokenization in languages other than English, we can load the respective language pickle file found in tokenizers/punkt and then tokenize the text in another language, which is an argument of the tokenize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britanniquedeLevallois-Perret.',\n",
       " 'Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois.',\n",
       " \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire.\",\n",
       " \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_tokenizer=nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "text1=\"Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britanniquedeLevallois-Perret. Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\"\n",
    "french_tokenizer.tokenize(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of sentences into words : TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'day.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'the',\n",
       " 'book',\n",
       " 'interesting',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"Have a nice day. I hope you find the book interesting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting punctuation WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\" Don't hesitate to ask questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using regular expressions(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', 't', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer.tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "sent=\"Don't hesitate to ask questions\"\n",
    "print(regexp_tokenize(sent, pattern='\\w+|\\$[\\d\\.]+|\\S+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion into lowercase and uppercase :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardwork is key to success\n"
     ]
    }
   ],
   "source": [
    "text='HARdWork IS KEy to SUCCESS'\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARDWORK IS KEY TO SUCCESS\n"
     ]
    }
   ],
   "source": [
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with stop words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'ask', 'questions']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "[word for word in words if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have', 'nice', 'day', 'I', 'hope', 'find', 'book', 'interesting']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "test1 = \"Have a nice day. I hope you find the book interesting.\"\n",
    "a = tokenizer.tokenize(test1)\n",
    "[word for word in a if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the replacement of a text with another text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do not hesistate to ask questions'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns): \n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            s = re.sub(pattern, repl, s) \n",
    "        return s\n",
    "    \n",
    "\n",
    "replacer=RegexpReplacer()\n",
    "replacer.replace(\"Don't hesistate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She must have gone to the market but she did not go'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"She must've gone to the market but she didn't go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function of RegexpReplacer.replace() is substituting every instance of a replacement pattern with its corresponding substitution pattern. Here, must've is replaced by must have and didn't is replaced by did not , since the replacement pattern in replacers.py has already been defined by tuple pairs, that is, (r'(\\w+)\\'ve', '\\g<1> have') and (r'(\\w+)n\\'t', '\\g<1> not') .\n",
    "We can not only perform the replacement of contractions; we can also substitute a token with any other token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process in which we transform the word into a form with a different word category. The word formed after lemmatization is entirely different. The built-in morphy() function is used for lemmatization in WordNetLemmatizer. The inputted word is left unchanged if it is not found in WordNet. In the argument, pos refers to the part of speech category of the inputted word. Consider an example of lemmatization in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'working'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output = WordNetLemmatizer()\n",
    "lemmatizer_output.lemmatize('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('working',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'child'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('children')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'consequence'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('consequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer_output=PorterStemmer()\n",
    "stemmer_output.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happiness'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import *\n",
    "edit_distance(\"relate\",\"relation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance(\"suggestion\",\"calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Jaccard(X,Y)=|X∩Y|/|XUY|\n",
    "• Jaccard(X,X)=1\n",
    "• Jaccard(X,Y)=0 if X∩Y=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "X=set([10,20,30,40])\n",
    "Y=set([20,30,60])\n",
    "print(jaccard_distance(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding 2 nearest sentences between 2 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_file = open('NBA1.txt', 'r', encoding='utf-8')\n",
    "file_to_string = open_file.read()\n",
    "type(file_to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We start by performing the replacement of contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We’ll find out\n",
      "\n",
      "“We’ll find out\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "text_replaced = replacer.replace(file_to_string)\n",
    "\n",
    "print(file_to_string[-156:-142])\n",
    "print(text_replaced[-158:-142])\n",
    "print(type(text_replaced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizing into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The NBA’s history in China is more than three decades old.',\n",
       " 'China Central Television struck a deal with the league in 1987 to offer games for free, and their relationship prospered in the 1990s, as the Chicago Bulls were busy winning championships and Michael Jordan was becoming a global icon.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = tokenizer.tokenize(text_replaced)\n",
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizing into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'NBA',\n",
       " 's',\n",
       " 'history',\n",
       " 'in',\n",
       " 'China',\n",
       " 'is',\n",
       " 'more',\n",
       " 'than',\n",
       " 'three',\n",
       " 'decades',\n",
       " 'old']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = tokenizer.tokenize(sentences[i])\n",
    "sentences[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We now delete stop words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'NBA',\n",
       " 's',\n",
       " 'history',\n",
       " 'in',\n",
       " 'China',\n",
       " 'is',\n",
       " 'more',\n",
       " 'than',\n",
       " 'three',\n",
       " 'decades',\n",
       " 'old']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'NBA', 'history', 'China', 'three', 'decades', 'old']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words= sentences[0]\n",
    "[word for word in words if word not in stops]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lemmatization for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'NBA',\n",
       "  's',\n",
       "  'history',\n",
       "  'in',\n",
       "  'China',\n",
       "  'is',\n",
       "  'more',\n",
       "  'than',\n",
       "  'three',\n",
       "  'decades',\n",
       "  'old'],\n",
       " ['China',\n",
       "  'Central',\n",
       "  'Television',\n",
       "  'struck',\n",
       "  'a',\n",
       "  'deal',\n",
       "  'with',\n",
       "  'the',\n",
       "  'league',\n",
       "  'in',\n",
       "  '1987',\n",
       "  'to',\n",
       "  'offer',\n",
       "  'games',\n",
       "  'for',\n",
       "  'free',\n",
       "  'and',\n",
       "  'their',\n",
       "  'relationship',\n",
       "  'prospered',\n",
       "  'in',\n",
       "  'the',\n",
       "  '1990s',\n",
       "  'as',\n",
       "  'the',\n",
       "  'Chicago',\n",
       "  'Bulls',\n",
       "  'were',\n",
       "  'busy',\n",
       "  'winning',\n",
       "  'championships',\n",
       "  'and',\n",
       "  'Michael',\n",
       "  'Jordan',\n",
       "  'was',\n",
       "  'becoming',\n",
       "  'a',\n",
       "  'global',\n",
       "  'icon'],\n",
       " ['The',\n",
       "  'NBA',\n",
       "  'has',\n",
       "  'only',\n",
       "  'become',\n",
       "  'more',\n",
       "  'popular',\n",
       "  'in',\n",
       "  'China',\n",
       "  'since',\n",
       "  'then'],\n",
       " ['The',\n",
       "  'league',\n",
       "  's',\n",
       "  'official',\n",
       "  'Chinese',\n",
       "  'language',\n",
       "  'account',\n",
       "  'on',\n",
       "  'Weibo',\n",
       "  'Inc',\n",
       "  's',\n",
       "  'short',\n",
       "  'messaging',\n",
       "  'service',\n",
       "  'has',\n",
       "  'more',\n",
       "  'followers',\n",
       "  'than',\n",
       "  'its',\n",
       "  'account',\n",
       "  'on',\n",
       "  'Twitter'],\n",
       " ['Floor',\n",
       "  'seats',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Lakers',\n",
       "  'vs',\n",
       "  'Nets',\n",
       "  'game',\n",
       "  'on',\n",
       "  'Thursday',\n",
       "  'were',\n",
       "  'being',\n",
       "  'sold',\n",
       "  'for',\n",
       "  'more',\n",
       "  'than',\n",
       "  '2',\n",
       "  '500',\n",
       "  'before',\n",
       "  'they',\n",
       "  'vanished',\n",
       "  'from',\n",
       "  'an',\n",
       "  'Alibaba',\n",
       "  'backed',\n",
       "  'ticketing',\n",
       "  'site',\n",
       "  'as',\n",
       "  'remnants',\n",
       "  'of',\n",
       "  'the',\n",
       "  'NBA',\n",
       "  'continued',\n",
       "  'to',\n",
       "  'disappear'],\n",
       " ['China',\n",
       "  'has',\n",
       "  '300',\n",
       "  'million',\n",
       "  'basketball',\n",
       "  'players',\n",
       "  'nearly',\n",
       "  'as',\n",
       "  'many',\n",
       "  'people',\n",
       "  'as',\n",
       "  'there',\n",
       "  'are',\n",
       "  'in',\n",
       "  'the',\n",
       "  'U',\n",
       "  'S',\n",
       "  'and',\n",
       "  'roughly',\n",
       "  '500',\n",
       "  'million',\n",
       "  'viewers',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'NBA',\n",
       "  'last',\n",
       "  'season',\n",
       "  'on',\n",
       "  'Tencent',\n",
       "  'Sports',\n",
       "  'the',\n",
       "  'streaming',\n",
       "  'platform',\n",
       "  'that',\n",
       "  'recently',\n",
       "  'extended',\n",
       "  'its',\n",
       "  'deal',\n",
       "  'with',\n",
       "  'the',\n",
       "  'league',\n",
       "  'for',\n",
       "  'more',\n",
       "  'than',\n",
       "  '1',\n",
       "  '5',\n",
       "  'billion',\n",
       "  'over',\n",
       "  'five',\n",
       "  'years'],\n",
       " ['That',\n",
       "  'deal',\n",
       "  'was',\n",
       "  'a',\n",
       "  'massive',\n",
       "  'indicator',\n",
       "  'for',\n",
       "  'the',\n",
       "  'perceived',\n",
       "  'value',\n",
       "  'and',\n",
       "  'enormous',\n",
       "  'potential',\n",
       "  'of',\n",
       "  'the',\n",
       "  'China',\n",
       "  'market',\n",
       "  'the',\n",
       "  'Shanghai',\n",
       "  'based',\n",
       "  'sports',\n",
       "  'research',\n",
       "  'firm',\n",
       "  'Mailman',\n",
       "  'Group',\n",
       "  'wrote',\n",
       "  'in',\n",
       "  'a',\n",
       "  'recent',\n",
       "  'report'],\n",
       " ['The',\n",
       "  'league',\n",
       "  's',\n",
       "  'carefully',\n",
       "  'plotted',\n",
       "  'strategy',\n",
       "  'over',\n",
       "  'the',\n",
       "  'course',\n",
       "  'of',\n",
       "  'more',\n",
       "  'than',\n",
       "  '30',\n",
       "  'years',\n",
       "  'has',\n",
       "  'given',\n",
       "  'the',\n",
       "  'NBA',\n",
       "  'a',\n",
       "  'negotiating',\n",
       "  'gambit',\n",
       "  'that',\n",
       "  'eluded',\n",
       "  'Hollywood',\n",
       "  'studios',\n",
       "  'Marriott',\n",
       "  'Gap',\n",
       "  'Delta',\n",
       "  'Airlines',\n",
       "  'and',\n",
       "  'other',\n",
       "  'fixtures',\n",
       "  'of',\n",
       "  'corporate',\n",
       "  'America',\n",
       "  'that',\n",
       "  'have',\n",
       "  'scrambled',\n",
       "  'to',\n",
       "  'appease',\n",
       "  'China'],\n",
       " ['The',\n",
       "  'NBA',\n",
       "  'has',\n",
       "  'leverage',\n",
       "  'in',\n",
       "  'China',\n",
       "  'if',\n",
       "  'it',\n",
       "  'works',\n",
       "  'as',\n",
       "  'a',\n",
       "  'united',\n",
       "  'front',\n",
       "  'Bishop',\n",
       "  'wrote',\n",
       "  'in',\n",
       "  'his',\n",
       "  'Sinocism',\n",
       "  'newsletter',\n",
       "  'this',\n",
       "  'week'],\n",
       " ['They', 'cannot', 'and', 'will', 'not', 'shun', 'an', 'entire', 'league'],\n",
       " ['Do',\n",
       "  'you',\n",
       "  'really',\n",
       "  'think',\n",
       "  'those',\n",
       "  'fans',\n",
       "  'are',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'satisfied',\n",
       "  'watching',\n",
       "  'CBA',\n",
       "  'games'],\n",
       " ['There',\n",
       "  'would',\n",
       "  'be',\n",
       "  'a',\n",
       "  'social',\n",
       "  'stability',\n",
       "  'cost',\n",
       "  'to',\n",
       "  'banning',\n",
       "  'the',\n",
       "  'NBA',\n",
       "  'in',\n",
       "  'China',\n",
       "  'The',\n",
       "  'Friday',\n",
       "  'night',\n",
       "  'tweet',\n",
       "  'by',\n",
       "  'Houston',\n",
       "  'Rockets',\n",
       "  'general',\n",
       "  'manager',\n",
       "  'Daryl',\n",
       "  'Morey',\n",
       "  'on',\n",
       "  'a',\n",
       "  'banned',\n",
       "  'platform',\n",
       "  'in',\n",
       "  'China',\n",
       "  'has',\n",
       "  'suddenly',\n",
       "  'thrust',\n",
       "  'the',\n",
       "  'NBA',\n",
       "  'into',\n",
       "  'the',\n",
       "  'turbulent',\n",
       "  'waters',\n",
       "  'of',\n",
       "  'Sino',\n",
       "  'American',\n",
       "  'politics'],\n",
       " ['The',\n",
       "  'NBA',\n",
       "  'represents',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'strongest',\n",
       "  'links',\n",
       "  'between',\n",
       "  'the',\n",
       "  'U',\n",
       "  'S',\n",
       "  'and',\n",
       "  'China',\n",
       "  'diplomats',\n",
       "  'and',\n",
       "  'business',\n",
       "  'executives',\n",
       "  'said',\n",
       "  'and',\n",
       "  'Morey',\n",
       "  's',\n",
       "  'tweet',\n",
       "  'about',\n",
       "  'the',\n",
       "  'protests',\n",
       "  'in',\n",
       "  'the',\n",
       "  'semiautonomous',\n",
       "  'city',\n",
       "  'of',\n",
       "  'Hong',\n",
       "  'Kong',\n",
       "  'has',\n",
       "  'called',\n",
       "  'attention',\n",
       "  'to',\n",
       "  'the',\n",
       "  'global',\n",
       "  'powers',\n",
       "  'disagreements',\n",
       "  'on',\n",
       "  'trade',\n",
       "  'cybersecurity',\n",
       "  'and',\n",
       "  'human',\n",
       "  'rights'],\n",
       " ['The',\n",
       "  'NBA',\n",
       "  'also',\n",
       "  'found',\n",
       "  'itself',\n",
       "  'on',\n",
       "  'the',\n",
       "  'wrong',\n",
       "  'end',\n",
       "  'of',\n",
       "  'another',\n",
       "  'backlash',\n",
       "  'in',\n",
       "  'the',\n",
       "  'U',\n",
       "  'S'],\n",
       " ['The',\n",
       "  'league',\n",
       "  's',\n",
       "  'initial',\n",
       "  'response',\n",
       "  'to',\n",
       "  'the',\n",
       "  'China',\n",
       "  'pushback',\n",
       "  'was',\n",
       "  'lambasted',\n",
       "  'by',\n",
       "  'American',\n",
       "  'politicians'],\n",
       " ['The',\n",
       "  'NBA',\n",
       "  's',\n",
       "  'response',\n",
       "  'managed',\n",
       "  'to',\n",
       "  'unite',\n",
       "  'elected',\n",
       "  'officials',\n",
       "  'on',\n",
       "  'the',\n",
       "  'left',\n",
       "  'and',\n",
       "  'right',\n",
       "  'from',\n",
       "  'Democratic',\n",
       "  'presidential',\n",
       "  'candidates',\n",
       "  'Elizabeth',\n",
       "  'Warren',\n",
       "  'and',\n",
       "  'Beto',\n",
       "  'O',\n",
       "  'Rourke',\n",
       "  'to',\n",
       "  'Republican',\n",
       "  'senators',\n",
       "  'Ted',\n",
       "  'Cruz',\n",
       "  'and',\n",
       "  'Marco',\n",
       "  'Rubio',\n",
       "  'who',\n",
       "  'attacked',\n",
       "  'the',\n",
       "  'league',\n",
       "  'for',\n",
       "  'appearing',\n",
       "  'to',\n",
       "  'value',\n",
       "  'profits',\n",
       "  'over',\n",
       "  'free',\n",
       "  'speech'],\n",
       " ['Silver',\n",
       "  'responded',\n",
       "  'on',\n",
       "  'Tuesday',\n",
       "  'For',\n",
       "  'those',\n",
       "  'who',\n",
       "  'question',\n",
       "  'our',\n",
       "  'motivations',\n",
       "  'this',\n",
       "  'is',\n",
       "  'about',\n",
       "  'far',\n",
       "  'more',\n",
       "  'than',\n",
       "  'growing',\n",
       "  'our',\n",
       "  'business',\n",
       "  'This',\n",
       "  'isn',\n",
       "  't',\n",
       "  'the',\n",
       "  'first',\n",
       "  'instance',\n",
       "  'of',\n",
       "  'frosty',\n",
       "  'relations',\n",
       "  'between',\n",
       "  'the',\n",
       "  'NBA',\n",
       "  'and',\n",
       "  'China'],\n",
       " ['CCTV',\n",
       "  'pulled',\n",
       "  'NBA',\n",
       "  'games',\n",
       "  'in',\n",
       "  '1999',\n",
       "  'after',\n",
       "  'a',\n",
       "  'U',\n",
       "  'S',\n",
       "  'led',\n",
       "  'coalition',\n",
       "  'of',\n",
       "  'bombers',\n",
       "  'destroyed',\n",
       "  'China',\n",
       "  's',\n",
       "  'embassy',\n",
       "  'in',\n",
       "  'Belgrade',\n",
       "  'in',\n",
       "  'what',\n",
       "  'Washington',\n",
       "  'claimed',\n",
       "  'was',\n",
       "  'a',\n",
       "  'mistake'],\n",
       " ['The',\n",
       "  'broadcaster',\n",
       "  'also',\n",
       "  'kept',\n",
       "  'NBA',\n",
       "  'games',\n",
       "  'off',\n",
       "  'the',\n",
       "  'air',\n",
       "  'longer',\n",
       "  'than',\n",
       "  'other',\n",
       "  'forms',\n",
       "  'of',\n",
       "  'entertainment',\n",
       "  'for',\n",
       "  'a',\n",
       "  'period',\n",
       "  'in',\n",
       "  '2008',\n",
       "  'following',\n",
       "  'a',\n",
       "  'number',\n",
       "  'of',\n",
       "  'criticisms',\n",
       "  'of',\n",
       "  'China',\n",
       "  'by',\n",
       "  'players'],\n",
       " ['But',\n",
       "  'people',\n",
       "  'around',\n",
       "  'the',\n",
       "  'Chinese',\n",
       "  'sports',\n",
       "  'industry',\n",
       "  'say',\n",
       "  'the',\n",
       "  'NBA',\n",
       "  's',\n",
       "  'success',\n",
       "  'in',\n",
       "  'China',\n",
       "  'is',\n",
       "  'the',\n",
       "  'result',\n",
       "  'of',\n",
       "  'its',\n",
       "  'sustained',\n",
       "  'effort',\n",
       "  'to',\n",
       "  'cultivate',\n",
       "  'a',\n",
       "  'fan',\n",
       "  'base',\n",
       "  'and',\n",
       "  'keep',\n",
       "  'government',\n",
       "  'authorities',\n",
       "  'pleased'],\n",
       " ['On',\n",
       "  'the',\n",
       "  'cusp',\n",
       "  'of',\n",
       "  'taking',\n",
       "  'power',\n",
       "  'in',\n",
       "  'China',\n",
       "  'in',\n",
       "  '2012',\n",
       "  'Xi',\n",
       "  'went',\n",
       "  'to',\n",
       "  'Los',\n",
       "  'Angeles',\n",
       "  'and',\n",
       "  'cheered',\n",
       "  'on',\n",
       "  'Kobe',\n",
       "  'Bryant',\n",
       "  'and',\n",
       "  'the',\n",
       "  'Lakers'],\n",
       " ['He',\n",
       "  's',\n",
       "  'remained',\n",
       "  'a',\n",
       "  'fan',\n",
       "  'since',\n",
       "  'then',\n",
       "  'and',\n",
       "  'he',\n",
       "  'officiated',\n",
       "  'at',\n",
       "  'the',\n",
       "  'FIBA',\n",
       "  'basketball',\n",
       "  'World',\n",
       "  'Cup',\n",
       "  's',\n",
       "  'opening',\n",
       "  'ceremony',\n",
       "  'in',\n",
       "  'August'],\n",
       " ['The',\n",
       "  'league',\n",
       "  'also',\n",
       "  'caters',\n",
       "  'to',\n",
       "  'Beijing',\n",
       "  's',\n",
       "  'desire',\n",
       "  'to',\n",
       "  'groom',\n",
       "  'a',\n",
       "  'next',\n",
       "  'generation',\n",
       "  'of',\n",
       "  'basketball',\n",
       "  'players',\n",
       "  'as',\n",
       "  'part',\n",
       "  'of',\n",
       "  'an',\n",
       "  'official',\n",
       "  'policy',\n",
       "  'to',\n",
       "  'build',\n",
       "  'China',\n",
       "  'into',\n",
       "  'a',\n",
       "  'leading',\n",
       "  'sports',\n",
       "  'nation',\n",
       "  'by',\n",
       "  'hosting',\n",
       "  'prospects',\n",
       "  'at',\n",
       "  'academies',\n",
       "  'from',\n",
       "  'eastern',\n",
       "  'Shandong',\n",
       "  'Province',\n",
       "  'to',\n",
       "  'the',\n",
       "  'northwestern',\n",
       "  'region',\n",
       "  'of',\n",
       "  'Xinjiang'],\n",
       " ['But',\n",
       "  'there',\n",
       "  'has',\n",
       "  'never',\n",
       "  'been',\n",
       "  'a',\n",
       "  'test',\n",
       "  'of',\n",
       "  'the',\n",
       "  'NBA',\n",
       "  's',\n",
       "  'relationship',\n",
       "  'with',\n",
       "  'China',\n",
       "  'like',\n",
       "  'this',\n",
       "  'one',\n",
       "  'and',\n",
       "  'the',\n",
       "  'uncertainty',\n",
       "  'is',\n",
       "  'only',\n",
       "  'increasing',\n",
       "  'with',\n",
       "  'both',\n",
       "  'sides',\n",
       "  'digging',\n",
       "  'in'],\n",
       " ['We',\n",
       "  'll',\n",
       "  'find',\n",
       "  'out',\n",
       "  'over',\n",
       "  'the',\n",
       "  'next',\n",
       "  'few',\n",
       "  'weeks',\n",
       "  'and',\n",
       "  'months',\n",
       "  'whether',\n",
       "  'the',\n",
       "  'idea',\n",
       "  'that',\n",
       "  'the',\n",
       "  'NBA',\n",
       "  'has',\n",
       "  'more',\n",
       "  'leverage',\n",
       "  'in',\n",
       "  'China',\n",
       "  'is',\n",
       "  'true',\n",
       "  'Bishop',\n",
       "  'said'],\n",
       " ['It', 'could', 'be', 'very', 'wrong']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['China',\n",
       " 'Central',\n",
       " 'Television',\n",
       " 'struck',\n",
       " 'a',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'the',\n",
       " 'league',\n",
       " 'in',\n",
       " '1987',\n",
       " 'to',\n",
       " 'offer',\n",
       " 'game',\n",
       " 'for',\n",
       " 'free',\n",
       " 'and',\n",
       " 'their',\n",
       " 'relationship',\n",
       " 'prospered',\n",
       " 'in',\n",
       " 'the',\n",
       " '1990s',\n",
       " 'a',\n",
       " 'the',\n",
       " 'Chicago',\n",
       " 'Bulls',\n",
       " 'were',\n",
       " 'busy',\n",
       " 'winning',\n",
       " 'championship',\n",
       " 'and',\n",
       " 'Michael',\n",
       " 'Jordan',\n",
       " 'wa',\n",
       " 'becoming',\n",
       " 'a',\n",
       " 'global',\n",
       " 'icon']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "        sentences[i][j] = lemmatizer_output.lemmatize(sentences[i][j])\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the words back into a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The NBA s history in China is more than three decade old'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = ' '.join(sentences[i])\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacer replace\n",
    "#Tokenize\n",
    "#Remove stopwords\n",
    "#Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacer replace\n",
    "open_file2 = open('NBA2.txt', 'r', encoding='utf-8')\n",
    "file_to_string2 = open_file2.read()\n",
    "text_replaced2 = replacer.replace(file_to_string2)\n",
    "\n",
    "#Tokenize\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences2 = tokenizer.tokenize(text_replaced2)\n",
    "\n",
    "#Tokenize words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "for i in range(len(sentences2)):\n",
    "    sentences2[i] = tokenizer.tokenize(sentences2[i])\n",
    "    \n",
    "#Remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(sentences2)):\n",
    "    sentences2[i] = [word for word in sentences2[i] if word not in stops]\n",
    "    \n",
    "#Lemmatize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(sentences2)):\n",
    "    for j in range(len(sentences2[i])):\n",
    "        sentences2[i][j] = lemmatizer_output.lemmatize(sentences2[i][j])\n",
    "\n",
    "        \n",
    "#Join the words back into a sentence.\n",
    "\n",
    "for i in range(len(sentences2)):\n",
    "    sentences2[i] = ' '.join(sentences2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SHANGHAI A day scheduled tipoff Brooklyn Nets Los Angeles Lakers game Shanghai crisis National Basketball Association appeared closer resolution'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text (text):\n",
    "    with open(text,'r',encoding=\"utf-8\") as f_open:\n",
    "        a=f_open.read()\n",
    "    \n",
    "    sentences_a=tokenizer.tokenize(a)\n",
    "    processed_a=[replacer.replace(sentence) for sentence in sentences_a]\n",
    "\n",
    "    wtokenizer = TreebankWordTokenizer()\n",
    "    w_processed_a =[wtokenizer.tokenize(sentence) for sentence in processed_a]\n",
    "\n",
    "    #Stop words\n",
    "    stops=set(stopwords.words('english'))\n",
    "    s_w_processed_a =[]\n",
    "    for i in range(len(w_processed_a)):\n",
    "        s_w_processed_a += [[wd for wd in w_processed_a[i] if wd not in stops]]\n",
    "    \n",
    "    \n",
    "\n",
    "    lm=WordNetLemmatizer()\n",
    "    lm_s_w_processed_a=[]\n",
    "    for i in range(len(s_w_processed_a)):\n",
    "        lm_s_w_processed_a += [[lm.lemmatize(wd) for wd in s_w_processed_a[i]]]\n",
    "    \n",
    "\n",
    "    \n",
    "    #Reconstruire mon texte avec les mots lmatizés\n",
    "    return [' '.join(s) for s in lm_s_w_processed_a] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to compare all sentences by measuring the similarity between two sentences using Jaccard's coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def jacard(a,b):\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\"\"\"\n",
    "def get_jaccard_sim(str1, str2): \n",
    "    a = set(str1.split()) \n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare all the sentences between both articles and keep the indexes of the sentences that got the best similarity (best Jaccard's coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import *\n",
    "\n",
    "comparison = []\n",
    "maximum, imax, jmax = 0.0, 0, 0\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences2)):\n",
    "        dist = get_jaccard_sim(sentences[i],sentences2[j])\n",
    "        if(dist > maximum):\n",
    "            maximum = dist\n",
    "            imax = i\n",
    "            jmax = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1346153846153846\n",
      "There would be a social stability cost to banning the NBA in China The Friday night tweet by Houston Rockets general manager Daryl Morey on a banned platform in China ha suddenly thrust the NBA into the turbulent water of Sino American politics\n",
      "Chinese broadcaster sponsor suspended aspect cooperation NBA commissioner said apologize tweet Rockets general manager Daryl Morey though league also said regrettable upset Chinese fan\n"
     ]
    }
   ],
   "source": [
    "print(maximum)\n",
    "print(sentences[imax])\n",
    "print(sentences2[jmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
